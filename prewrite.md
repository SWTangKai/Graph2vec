# Graph Embedding

## Introduce

## Relate Works

### Network Represent Learing

将网络记作`$G = (V,E)$`, 其中 `$V$` 是节点集合, E 是边的集合. 边 `$e = (v_i,v_j) \in E $`表示了节点 v_i 到 vj 的一条边. 网络的邻接矩阵定义为 A ∈ R|V|×|V|, 其中 Aij = 1 如果 (vi,vj) ∈ E, 否则 Aij = 0. 邻 接矩阵是网络数据的一种简单直接的表达形式. 邻接矩阵 A 的每一行表示了一个节点和所有其他节 点的链接关系, 可以看作是对应节点的一种表示

虽然方便直接, 使用邻接矩阵的网络表示受到计算效率问题的影响. 邻接矩阵 A 占用了 |V|×|V| 的存储空间, 这在 |V| 增长到百万级时通常是不可接受的. 另一方面, 邻接矩阵中绝大多数是 0, 数据 十分稀疏. 这种数据稀疏性使得快速有效的统计学习方法的应用变得困难 [1].

因此, 研究者们转而为网络中的节点学习低维稠密的向量表示. 形式化地, 网络表示学习的目标 就是对每个节点 v ∈ V 学习一个实数向量 Rv ∈ Rk, 其中向量的维度 k 远远小于节点的总个数 |V|. 网络表示学习的过程可以是无监督或者半监督的. 通过优化算法自动得到而不需要特征工程的节点表 示可以进一步用于后续的网络应用任务, 如节点分类. 这些低维的向量表示使得快速高效的算法设计 成为可能, 而不必再去考虑原本的网络结构.

Embed using Random Work

* Deepwork
* Node2vec

Embed using one and two qapprox

* Line

Struc2vec

### visualization Graph
